{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 이미지 임포트\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# tf.constant로 상수를 Hello 변수에 저장\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "print(hello)\n",
    "\n",
    "# hello가 텐서플로의 Tensor 라는 자료형이고 상수를 담고 있음. 랭크와 셰이프 개념.   셰이프는 [1,2,3] 이 한 묶음. 랭크는 이를 밖에서 묶은 개념으로 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(10)\n",
    "b = tf.constant(32)\n",
    "c = tf.add(a, b)\n",
    "print(c)\n",
    "\n",
    "# 그래프는 텐서들의 연산 모음으로 c=tf.add는 지연실행(lazy evaluation) 된 상황.\n",
    "# 아래 sess.close() 에서 실제 실행 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n",
      "[10, 32, 42]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "print(sess.run(hello))\n",
    "print(sess.run([a, b, c]))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기초2. 플레이스홀더와 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_1:0\", shape=(?, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 그래프에 사용할 입력값을 나중에 받기 위해 사용하는 매개변수(Parameter)이다. \n",
    "# 변수는 그래프를 최적화하는 용도로 텐서플로가 학습한 결과를 갱신하기 위해 사용하는 변수\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 3])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1,2,3],[4,5,6]]   # 랭크가 2, 셰이프는 2,3 (중괄호 안에 들어있는 인자 개수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal([3,2]))\n",
    "b = tf.Variable(tf.random_normal([2,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable([[0.1, 0.1], [0.2, 0.2], [0.3, 0.3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = tf.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== x_data ===\n",
      "[[1, 2, 3], [4, 5, 6]]\n",
      "=== W ===\n",
      "[[0.1 0.1]\n",
      " [0.2 0.2]\n",
      " [0.3 0.3]]\n",
      "=== b ===\n",
      "[[-0.5961132 ]\n",
      " [-0.16502537]]\n",
      "=== expr ===\n",
      "[[0.8038869 0.8038869]\n",
      " [3.0349746 3.0349746]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())   # 앞에서 정의한 변수들을 초기화하는 함수. 기존에 학습한 값들을 가져와서 실행하는 게 아니라면 연산 실행 전에 반드시 이 함수를 이용해 변수들을 초기화 해야 한다.\n",
    "\n",
    "print(\"=== x_data ===\")\n",
    "print(x_data)\n",
    "print(\"=== W ===\")\n",
    "print(sess.run(W))\n",
    "print(\"=== b ===\")\n",
    "print(sess.run(b))\n",
    "print(\"=== expr ===\")\n",
    "print(sess.run(expr, feed_dict={X: x_data}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연습. 선형 회귀 모델 구현하기\n",
    " - 선형 회귀란 간단하게 말해, 주어진 x와 y의 값을 가지고 서로간의 관계를 파악하는 것\n",
    " - 이 관계를 알고 나면 새로운 x 값이 주어졌을 때 y 값을 쉽게 알수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1, 2, 3]\n",
    "y_data = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, name=\"X\")\n",
    "Y = tf.placeholder(tf.float32, name=\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = W * X + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수(loss function) : 한 쌍(x, y)의 데이터에 대한 손실값을 계산하는 함수\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent(경사하강법) 최적화 Method를 이용해 손실값을 최소화 하는 연산그래프\n",
    "#  -> 최적화 함수란 가중치와 편향 값을 변경해가면서 손실값을 최소화하는 가장 최적화된 가중치와 편향값을 찾아주는 함수.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1) # 학습률과 같이 학습을 진행하는 과정에서 영향을 주는 변수를 '하이퍼파라미터' 라 한다.\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "# 즉, 함수의 기울기를 구하고 기울기가 낮은 쪽으로 계속 이동시키면서 최적화의 값을 찾아나가는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7.597962 [0.8770337] [0.5962786]\n",
      "1 0.13282286 [0.7532908] [0.5262094]\n",
      "2 0.041652214 [0.77306896] [0.51965123]\n",
      "3 0.038660012 [0.77701074] [0.5064934]\n",
      "4 0.036811527 [0.7825367] [0.4943904]\n",
      "5 0.0350628 [0.7877463] [0.48249763]\n",
      "6 0.033397287 [0.79285073] [0.47089958]\n",
      "7 0.03181089 [0.7978302] [0.45957938]\n",
      "8 0.030299827 [0.8026902] [0.4485314]\n",
      "9 0.028860597 [0.8074335] [0.43774903]\n",
      "10 0.027489712 [0.8120627] [0.42722583]\n",
      "11 0.026183913 [0.81658053] [0.4169556]\n",
      "12 0.024940133 [0.8209898] [0.40693226]\n",
      "13 0.023755474 [0.82529306] [0.3971499]\n",
      "14 0.02262708 [0.8294929] [0.3876027]\n",
      "15 0.021552265 [0.8335918] [0.378285]\n",
      "16 0.020528538 [0.8375922] [0.3691913]\n",
      "17 0.019553393 [0.8414962] [0.36031613]\n",
      "18 0.018624606 [0.84530663] [0.3516544]\n",
      "19 0.017739927 [0.84902537] [0.34320086]\n",
      "20 0.016897233 [0.85265464] [0.33495054]\n",
      "21 0.01609463 [0.85619676] [0.32689857]\n",
      "22 0.015330125 [0.8596537] [0.31904015]\n",
      "23 0.014601931 [0.8630276] [0.31137064]\n",
      "24 0.013908318 [0.86632025] [0.3038855]\n",
      "25 0.013247655 [0.86953384] [0.29658028]\n",
      "26 0.012618379 [0.8726701] [0.2894507]\n",
      "27 0.012019019 [0.87573105] [0.28249252]\n",
      "28 0.011448108 [0.8787184] [0.27570158]\n",
      "29 0.0109043 [0.88163394] [0.26907393]\n",
      "30 0.010386339 [0.8844794] [0.26260558]\n",
      "31 0.009892982 [0.8872564] [0.2562927]\n",
      "32 0.009423068 [0.88996667] [0.2501316]\n",
      "33 0.008975469 [0.8926118] [0.24411862]\n",
      "34 0.008549109 [0.89519334] [0.23825017]\n",
      "35 0.008143022 [0.89771277] [0.23252279]\n",
      "36 0.007756228 [0.90017176] [0.22693312]\n",
      "37 0.007387804 [0.90257156] [0.2214778]\n",
      "38 0.007036887 [0.90491366] [0.21615364]\n",
      "39 0.0067026173 [0.90719944] [0.21095745]\n",
      "40 0.0063842335 [0.90943027] [0.20588619]\n",
      "41 0.0060809813 [0.91160756] [0.20093685]\n",
      "42 0.005792134 [0.9137324] [0.19610645]\n",
      "43 0.0055170073 [0.9158063] [0.19139221]\n",
      "44 0.0052549397 [0.9178302] [0.18679126]\n",
      "45 0.005005332 [0.9198055] [0.18230093]\n",
      "46 0.0047675674 [0.9217333] [0.17791851]\n",
      "47 0.0045411116 [0.92361486] [0.17364149]\n",
      "48 0.0043254034 [0.9254511] [0.16946727]\n",
      "49 0.004119942 [0.9272432] [0.16539337]\n",
      "50 0.003924247 [0.9289922] [0.16141742]\n",
      "51 0.003737835 [0.93069917] [0.15753706]\n",
      "52 0.0035602862 [0.9323651] [0.15374997]\n",
      "53 0.0033911753 [0.933991] [0.15005393]\n",
      "54 0.0032300816 [0.9355778] [0.14644673]\n",
      "55 0.0030766607 [0.9371265] [0.14292626]\n",
      "56 0.0029305136 [0.938638] [0.13949043]\n",
      "57 0.002791308 [0.94011307] [0.13613716]\n",
      "58 0.0026587173 [0.94155264] [0.13286449]\n",
      "59 0.0025324288 [0.9429577] [0.12967055]\n",
      "60 0.002412134 [0.94432896] [0.12655336]\n",
      "61 0.002297563 [0.94566727] [0.12351111]\n",
      "62 0.0021884225 [0.9469734] [0.12054198]\n",
      "63 0.0020844685 [0.9482481] [0.11764423]\n",
      "64 0.001985458 [0.94949216] [0.11481614]\n",
      "65 0.0018911464 [0.95070636] [0.11205606]\n",
      "66 0.0018013185 [0.95189136] [0.10936232]\n",
      "67 0.001715752 [0.9530479] [0.10673331]\n",
      "68 0.0016342504 [0.9541765] [0.10416749]\n",
      "69 0.0015566262 [0.9552781] [0.10166339]\n",
      "70 0.0014826875 [0.9563532] [0.09921947]\n",
      "71 0.001412253 [0.9574024] [0.09683429]\n",
      "72 0.0013451738 [0.9584265] [0.09450649]\n",
      "73 0.0012812758 [0.9594258] [0.0922346]\n",
      "74 0.0012204169 [0.96040124] [0.09001736]\n",
      "75 0.0011624467 [0.9613531] [0.08785339]\n",
      "76 0.0011072237 [0.9622822] [0.08574147]\n",
      "77 0.0010546338 [0.9631889] [0.08368029]\n",
      "78 0.001004535 [0.9640738] [0.08166867]\n",
      "79 0.0009568175 [0.96493745] [0.07970542]\n",
      "80 0.00091137184 [0.9657803] [0.07778935]\n",
      "81 0.00086808065 [0.9666029] [0.07591935]\n",
      "82 0.0008268445 [0.96740574] [0.07409429]\n",
      "83 0.0007875687 [0.96818936] [0.07231315]\n",
      "84 0.00075015984 [0.968954] [0.07057478]\n",
      "85 0.0007145296 [0.96970034] [0.0688782]\n",
      "86 0.0006805857 [0.97042876] [0.06722243]\n",
      "87 0.00064825785 [0.9711396] [0.06560644]\n",
      "88 0.00061746663 [0.9718334] [0.06402931]\n",
      "89 0.0005881364 [0.9725105] [0.0624901]\n",
      "90 0.0005601993 [0.97317135] [0.06098789]\n",
      "91 0.00053358515 [0.9738162] [0.05952175]\n",
      "92 0.00050824275 [0.9744457] [0.05809091]\n",
      "93 0.0004841025 [0.97506005] [0.05669446]\n",
      "94 0.00046110377 [0.97565955] [0.05533154]\n",
      "95 0.0004392042 [0.9762447] [0.05400142]\n",
      "96 0.0004183416 [0.97681576] [0.05270327]\n",
      "97 0.0003984679 [0.97737306] [0.0514363]\n",
      "98 0.00037954212 [0.9779171] [0.05019983]\n",
      "99 0.00036151512 [0.9784479] [0.04899305]\n",
      "\n",
      "===Test===\n",
      "X: 5, Y: [4.9412327]\n",
      "X: 2.5, Y: [2.495113]\n"
     ]
    }
   ],
   "source": [
    "# 세션을 생성하고 초기화합니다.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # 최적화를 100번 수행합니다.\n",
    "    for step in range(100):\n",
    "        # sess.run 을 통해 train_op 와 cost 그래프를 계산합니다.\n",
    "        # 이 때, 가설 수식에 넣어야 할 실제값을 feed_dict 을 통해 전달합니다.\n",
    "        _, cost_val = sess.run([train_op, cost], feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "        print(step, cost_val, sess.run(W), sess.run(b))\n",
    "\n",
    "\n",
    "    # 최적화가 완료된 모델에 테스트 값을 넣고 결과가 잘 나오는지 확인해봅니다.\n",
    "    # X가 5, 2.5 일 때 Y는 몇인가를 예측해서 출력.\n",
    "    print(\"\\n===Test===\")\n",
    "    print(\"X: 5, Y:\", sess.run(hypothesis, feed_dict={X: 5}))\n",
    "    print(\"X: 2.5, Y:\", sess.run(hypothesis, feed_dict={X: 2.5}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
