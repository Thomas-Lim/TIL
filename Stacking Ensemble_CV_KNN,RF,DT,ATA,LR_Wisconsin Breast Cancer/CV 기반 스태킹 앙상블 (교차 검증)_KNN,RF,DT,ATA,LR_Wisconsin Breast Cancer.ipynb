{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.10.2 CV 기반 스태킹 앙상블 (교차 검증)\n",
    "- 일반 스태킹 앙상블의 과적합 문제를 해결\n",
    "- 개별 모델들이 각각 교차 검증(K-Fold)으로 메타 모델을 위한 학습용 스태킹 데이터 생성과 예측을 위한 테스트용 스태킹 데이터를 생성한 뒤 이를 기반으로 메타 모델이 학습과 예측을 수행.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 스텝 1. 각 모델별로 원본 학습/테스트 데이터를 예측한 결과 값을 기반으로 메타 모델을 위한 학습용/테스트용 데이터를 생성\n",
    "\n",
    "그림\n",
    "\n",
    "## 스텝 2. 위에서 생성된 학습용 데이터를 모두 스태킹 형태로 합쳐서 메타 모델이 학습할 최종 학습용 데이터 세트 생성. 메타 모델은 최종적으로 생성된 학습 데이터 세트와 원본 학습 데이터의 레이블 데이터를 기반으로 학습한 뒤, 최종적으로 생성된 테스트 데이터 세트를 예측하고 -> 원본 테스트 데이터의 y를 기반으로 평가\n",
    "\n",
    "그림"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 위스콘신 암 데이터 세트에 '스태킹 앙상블 모델' 적용해보기\n",
    "\n",
    "\n",
    "\n",
    "# get_stacking_base_datasets() 함수에 이하 입력\n",
    "  1) 각 모델, \n",
    "  2) 원본 train 피처및 y, test 피처 Data, \n",
    "  3) K-Fold 개수\n",
    "  \n",
    "- 함수 내에서 Fold의 개수만큼 반복을 수행하면서 Fold된 학습용 데이터로 학습한 뒤 예측 결괏값을 기반으로 메타 모델을 위한 Train Data, Test Data 새롭게 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "# 개별 기반 모델에서 최종 메타 모델이 사용할 학습 및 테스트용 데이터 생성 위한 함수.\n",
    "def get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds):\n",
    "    # 지정된 n_folds값으로 KFold 생성\n",
    "    kf = KFold(n_splits=n_folds, shuffle=False, random_state=0)\n",
    "    \n",
    "    # 추후 메타 모델이 사용할 학습 데이터 반환을 위한 numpy 배열 초기화\n",
    "    train_fold_pred = np.zeros((X_train_n.shape[0], 1 ))\n",
    "    test_pred = np.zeros((X_test_n.shape[0], n_folds))\n",
    "    print(model.__class__.__name__, ' model 시작')\n",
    "    # DecisionTreeClassifier  model 시작  이런식으로 표현됨\n",
    "    \n",
    "    \n",
    "    for folder_counter, (train_index, valid_index) in enumerate(kf.split(X_train_n)):\n",
    "        # 입력된 학습 데이터에서 기반 모델이 학습/예측할 Fold 데이터 세트 추출\n",
    "        print('\\t 폴드 세트: ', folder_counter, '시작')\n",
    "        X_tr = X_train_n[train_index]\n",
    "        y_tr = y_train_n[train_index]\n",
    "        X_te = X_train_n[valid_index]\n",
    "        \n",
    "        \n",
    "        # 폴드 세트 내부에서 다시 만들어진 학습 데이터로 기반 모델의 학습 수행.\n",
    "        model.fit(X_tr, y_tr)\n",
    "        \n",
    "        # 폴드 세트 내부에서 다시 만들어진 검증 데이터로 기반 모델 예측 후 데이터 저장.\n",
    "        train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1 ,1)\n",
    "        \n",
    "        # 입력된 원본 테스트 데이터를 폴드 세트내 학습된 기반 모델에서 예측 후 데이터 저장.\n",
    "        test_pred[:, folder_counter] = model.predict(X_test_n)\n",
    "        \n",
    "    # 폴드 세트 내에서 원본 테스트 데이터를 예측한 데이터를 평균하여 테스트 데이터로 생성\n",
    "    test_pred_mean = np.mean(test_pred, axis=1).reshape(-1,1)\n",
    "    \n",
    "    # train_fold_pred는 최종 메타 모델이 사용하는 학습 데이터, test_pred_mean은 최종 메타 모델이 사용하는 테스트 데이터.\n",
    "    return train_fold_pred, test_pred_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 위 4가지 모델의 예측 결과를 합한 데이터 세트로 학습/예측하는 최종 모델은 아래의 로지스틱 회귀이다.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cancer_data = load_breast_cancer()\n",
    "\n",
    "X_data = cancer_data.data\n",
    "y_label = cancer_data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_label, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN 정확도: 0.9211\n",
      "랜덤 포레스트 정확도: 0.9649\n",
      "DT 정확도: 0.9035\n",
      "ADA 정확도: 0.9561\n"
     ]
    }
   ],
   "source": [
    "knn_clf = KNeighborsClassifier(n_neighbors=4)\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "ada_clf = AdaBoostClassifier(n_estimators=100)\n",
    "\n",
    "\n",
    "# 스태킹으로 만들어진 데이터 세트를 학습, 예측할 최종 모델\n",
    "lr_final = LogisticRegression(C=10)\n",
    "\n",
    "\n",
    "\n",
    "# 학습\n",
    "\n",
    "knn_clf.fit(X_train,y_train)\n",
    "rf_clf.fit(X_train,y_train)\n",
    "dt_clf.fit(X_train,y_train)  \n",
    "ada_clf.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "# 예측\n",
    "knn_pred = knn_clf.predict(X_test)\n",
    "rf_pred = rf_clf.predict(X_test)\n",
    "dt_pred = dt_clf.predict(X_test)\n",
    "ada_pred = ada_clf.predict(X_test)\n",
    "# gbm_pred = gbm_clf.predict(X_test)\n",
    "\n",
    "print('KNN 정확도: {0:.4f}'.format(accuracy_score(y_test, knn_pred)))\n",
    "print('랜덤 포레스트 정확도: {0:.4f}'.format(accuracy_score(y_test, rf_pred)))\n",
    "print('DT 정확도: {0:.4f}'.format(accuracy_score(y_test, dt_pred)))\n",
    "print('ADA 정확도: {0:.4f}'.format(accuracy_score(y_test, ada_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 스텝1 구현\n",
    "- 모델별로 get_stacking_base_datasets() 함수를 호출해 각각 메타 모델이 추후에 사용할 학습용, 테스트용 데이터 세트 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier  model 시작\n",
      "\t 폴드 세트:  0 시작\n",
      "\t 폴드 세트:  1 시작\n",
      "\t 폴드 세트:  2 시작\n",
      "\t 폴드 세트:  3 시작\n",
      "\t 폴드 세트:  4 시작\n",
      "\t 폴드 세트:  5 시작\n",
      "\t 폴드 세트:  6 시작\n",
      "RandomForestClassifier  model 시작\n",
      "\t 폴드 세트:  0 시작\n",
      "\t 폴드 세트:  1 시작\n",
      "\t 폴드 세트:  2 시작\n",
      "\t 폴드 세트:  3 시작\n",
      "\t 폴드 세트:  4 시작\n",
      "\t 폴드 세트:  5 시작\n",
      "\t 폴드 세트:  6 시작\n",
      "DecisionTreeClassifier  model 시작\n",
      "\t 폴드 세트:  0 시작\n",
      "\t 폴드 세트:  1 시작\n",
      "\t 폴드 세트:  2 시작\n",
      "\t 폴드 세트:  3 시작\n",
      "\t 폴드 세트:  4 시작\n",
      "\t 폴드 세트:  5 시작\n",
      "\t 폴드 세트:  6 시작\n",
      "AdaBoostClassifier  model 시작\n",
      "\t 폴드 세트:  0 시작\n",
      "\t 폴드 세트:  1 시작\n",
      "\t 폴드 세트:  2 시작\n",
      "\t 폴드 세트:  3 시작\n",
      "\t 폴드 세트:  4 시작\n",
      "\t 폴드 세트:  5 시작\n",
      "\t 폴드 세트:  6 시작\n"
     ]
    }
   ],
   "source": [
    "knn_train, knn_test = get_stacking_base_datasets(knn_clf, X_train, y_train, X_test, 7)\n",
    "rf_train, rf_test = get_stacking_base_datasets(rf_clf, X_train, y_train, X_test, 7)\n",
    "dt_train, dt_test = get_stacking_base_datasets(dt_clf, X_train, y_train, X_test, 7)\n",
    "ada_train, ada_test = get_stacking_base_datasets(ada_clf, X_train, y_train, X_test, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 스텝2 구현\n",
    "- 위의 각 모델별 Train, Test 데이터를 합침. 넘파이의 concatenate()를 이용해 합침\n",
    "* concatenate()는 여러 개의 넘파이 배열을 칼럼 또는 로우 레벨로 합쳐줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 학습 피처 데이터 Shape: (455, 30) 원본 테스트 피처 Shape: (114, 30)\n",
      "스태킹 학습 피처 데이터 Shape: (455, 4) 스태킹 테스트 피처 데이터 Shape: (114, 4)\n"
     ]
    }
   ],
   "source": [
    "Stack_final_X_train = np.concatenate((knn_train, rf_train, dt_train, ada_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((knn_test, rf_test, dt_test, ada_test), axis=1)\n",
    "print('원본 학습 피처 데이터 Shape:', X_train.shape, '원본 테스트 피처 Shape:', X_test.shape)\n",
    "print('스태킹 학습 피처 데이터 Shape:', Stack_final_X_train.shape,\n",
    "      '스태킹 테스트 피처 데이터 Shape:', Stack_final_X_test.shape)\n",
    "\n",
    "\n",
    "# 이렇게 만들어진 Stack_final_X_train,test는 메타 모델이 학습할 학습용 , 예측 테스트용 피처 데이터 세트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 최종 메타 모델인 LR를 스태킹된 학습용 피처 데이터 세트와 원본 학습 레이블 데이터로 학습\n",
    "## 2) 스태킹 된 테스트 데이터를 세트로 예측\n",
    "## 3) 예측 결과를 원본 테스트 레이블 데이터와 비교해 정확도 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 메타 모델의 예측 정확도: 0.9737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lr_final.fit(Stack_final_X_train, y_train)\n",
    "stack_final = lr_final.predict(Stack_final_X_test)\n",
    "\n",
    "print('최종 메타 모델의 예측 정확도: {0:.4f}'.format(accuracy_score(y_test, stack_final)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정리.\n",
    "\n",
    "# 1. 스태킹은 일반으로 여러 명으로 이뤄진 분석 팀에서 개별적으로 각각 모델을 최적으로 학습시켜서 스태킹 모델을 더 빠르게 최적화 하는 파라미터 튜닝 작업이 우선되야함.\n",
    "\n",
    "# 2. 스태킹 모델은 분류 뿐 아니라 회귀에서도 적용 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
